# DAY14-NOTE

## 前向传播

### 人工神经元

人工神经元是受生物神经元启发设计的数学模型。一个生物神经元通过树突接收信号，在细胞体进行处理，然后通过轴突传递输出。人工神经元模拟了这一过程：

生物神经元：  树突（输入）→ 细胞体（处理）→ 轴突（输出）

人工神经元：  输入特征 → 加权求和 + 偏置 → 激活函数 → 输出

### 单个神经元的前向传播数学原理

基本计算过程：

一个神经元接收多个输入，每个输入都有对应的权重，加上偏置后通过激活函数输出：

z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b

a = f(z)

其中：

x₁, x₂, ..., xₙ：输入特征（来自数据或上一层）

w₁, w₂, ..., wₙ：权重（可学习参数）

b：偏置（可学习参数）

z：净输入（加权和 + 偏置）

f：激活函数（引入非线性）

a：神经元的输出

向量化表达：

z = WᵀX + b

a = f(z)

X = [x₁, x₂, ..., xₙ]ᵀ （输入向量）

W = [w₁, w₂, ..., wₙ]ᵀ （权重向量）

EXAMPLE：假设一个神经元有3个输入：

```python
x₁ = 0.5, x₂ = 0.8, x₃ = 0.2
w₁ = 0.1, w₂ = 0.4, w₃ = 0.3
b = -0.2
激活函数：ReLU

计算：
z = 0.5×0.1 + 0.8×0.4 + 0.2×0.3 + (-0.2)
  = 0.05 + 0.32 + 0.06 - 0.2
  = 0.23
a = ReLU(0.23) = 0.23  （因为大于0）
```

### 激活函数

【1】神经网络训练分两步：

1.前向传播：用激活函数 σ(x) 算出输出

2.反向传播：用梯度（导数） σ’(x) 算误差该怎么传回去、权重该怎么更新

梯度函数，就是激活函数的导数。

没有激活函数，无论多少层神经网络都等价于单层线性变换。

【2】 为什么必须要有？

因为神经网络靠梯度下降学习。梯度下降要算：

我这个神经元稍微变一点，对最终误差影响多大？

这个 “影响” 就是 导数（梯度）。没有导数，就不知道往哪改权重，网络就学不会。

```python
假设没有激活函数：
第1层：a₁ = W₁x + b₁
第2层：a₂ = W₂a₁ + b₂ = W₂(W₁x + b₁) + b₂ = (W₂W₁)x + (W₂b₁ + b₂)

结果仍然是线性变换：a₂ = W'x + b'

激活函数的作用：

1.引入非线性，使网络能学习复杂模式

2.将神经元的输出限制在特定范围

3.帮助梯度流动（取决于具体函数）

```

#### 主流激活函数

##### （1）Sigmoid 函数：σ(x) = 1 / (1 + e⁻ˣ)

```python
图像特征：

输出范围：(0, 1)

S形曲线，中间线性，两端饱和

优点：

平滑，处处可导

输出可解释为概率

适合二分类输出层

缺点：

梯度消失：两端梯度趋近0

不是零中心化（输出恒正）

指数计算开销大

梯度公式：

text
σ'(x) = σ(x)(1 - σ(x))

```

##### （2）Tanh 函数（双曲正切）:tanh(x) = (eˣ - e⁻ˣ) / (eˣ + e⁻ˣ)
```python
图像特征：

输出范围：(-1, 1)

零中心化

优点：

零中心化，利于下一层学习

比Sigmoid梯度更强

缺点：

仍有梯度消失问题

计算开销大

梯度公式：

text
tanh'(x) = 1 - tanh²(x)

```

##### （3）ReLU（Rectified Linear Unit）:ReLU(x) = max(0, x)⭐⭐⭐ 最常用
```python
图像特征：

x > 0：输出 = x

x ≤ 0：输出 = 0

优点：

计算极快：只需比较大小

缓解梯度消失：正区间梯度恒为1

稀疏激活（部分神经元输出0）

缺点：

神经元死亡：负区间梯度0，参数永不更新

不是零中心化

梯度公式：

text
ReLU'(x) = 1  (x > 0)
ReLU'(x) = 0  (x ≤ 0)
```

##### （4）Leaky ReLU:LeakyReLU(x) = max(αx, x)，α通常取0.01

目的：解决ReLU死亡问题，给负区间一个小梯度

##### （5）Softmax（专用于多分类输出层）:softmax(z_i) = e^{z_i} / Σⱼ₌₁ᴷ e^{z_j}

特点：

输出和为1，形成概率分布

放大差异（指数运算）

只用于输出层

#### 激活函数选择指南


##### 应用场景	         推荐激活函数         原因

隐藏层（默认）	   ReLU	               计算快，效果好

隐藏层（防死亡） 	 Leaky ReLU	         解决神经元死亡

二分类输出层	     Sigmoid	           输出概率

多分类输出层	     Softmax	           输出概率分布

回归输出层	       无/线性	             输出任意实数

RNN/LSTM	       Tanh	               控制信息流动
