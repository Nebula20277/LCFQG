# DAY14-NOTE

## 前向传播

### 人工神经元

人工神经元是受生物神经元启发设计的数学模型。一个生物神经元通过树突接收信号，在细胞体进行处理，然后通过轴突传递输出。人工神经元模拟了这一过程：

生物神经元：  树突（输入）→ 细胞体（处理）→ 轴突（输出）

人工神经元：  输入特征 → 加权求和 + 偏置 → 激活函数 → 输出

### 单个神经元的前向传播数学原理

基本计算过程：

一个神经元接收多个输入，每个输入都有对应的权重，加上偏置后通过激活函数输出：

z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b

a = f(z)

其中：

x₁, x₂, ..., xₙ：输入特征（来自数据或上一层）

w₁, w₂, ..., wₙ：权重（可学习参数）

b：偏置（可学习参数）

z：净输入（加权和 + 偏置）

f：激活函数（引入非线性）

a：神经元的输出

向量化表达：

z = WᵀX + b

a = f(z)

X = [x₁, x₂, ..., xₙ]ᵀ （输入向量）

W = [w₁, w₂, ..., wₙ]ᵀ （权重向量）

EXAMPLE：假设一个神经元有3个输入：

```python
x₁ = 0.5, x₂ = 0.8, x₃ = 0.2
w₁ = 0.1, w₂ = 0.4, w₃ = 0.3
b = -0.2
激活函数：ReLU

计算：
z = 0.5×0.1 + 0.8×0.4 + 0.2×0.3 + (-0.2)
  = 0.05 + 0.32 + 0.06 - 0.2
  = 0.23
a = ReLU(0.23) = 0.23  （因为大于0）
```

### 激活函数

【1】神经网络训练分两步：

1.前向传播：用激活函数 σ(x) 算出输出

2.反向传播：用梯度（导数） σ’(x) 算误差该怎么传回去、权重该怎么更新

梯度函数，就是激活函数的导数。

没有激活函数，无论多少层神经网络都等价于单层线性变换。

【2】 为什么必须要有？

因为神经网络靠梯度下降学习。梯度下降要算：

我这个神经元稍微变一点，对最终误差影响多大？

这个 “影响” 就是 导数（梯度）。没有导数，就不知道往哪改权重，网络就学不会。

```python
假设没有激活函数：
第1层：a₁ = W₁x + b₁
第2层：a₂ = W₂a₁ + b₂ = W₂(W₁x + b₁) + b₂ = (W₂W₁)x + (W₂b₁ + b₂)

结果仍然是线性变换：a₂ = W'x + b'

激活函数的作用：

1.引入非线性，使网络能学习复杂模式

2.将神经元的输出限制在特定范围

3.帮助梯度流动（取决于具体函数）

```

#### 主流激活函数

##### （1）Sigmoid 函数：σ(x) = 1 / (1 + e⁻ˣ)

```python
图像特征：

输出范围：(0, 1)

S形曲线，中间线性，两端饱和

优点：

平滑，处处可导

输出可解释为概率

适合二分类输出层

缺点：

梯度消失：两端梯度趋近0

不是零中心化（输出恒正）

指数计算开销大

梯度公式：

text
σ'(x) = σ(x)(1 - σ(x))

```

##### （2）Tanh 函数（双曲正切）:tanh(x) = (eˣ - e⁻ˣ) / (eˣ + e⁻ˣ)
```python
图像特征：

输出范围：(-1, 1)

零中心化

优点：

零中心化，利于下一层学习

比Sigmoid梯度更强

缺点：

仍有梯度消失问题

计算开销大

梯度公式：

text
tanh'(x) = 1 - tanh²(x)

```

##### （3）ReLU（Rectified Linear Unit）:ReLU(x) = max(0, x)⭐⭐⭐ 最常用
```python
图像特征：

x > 0：输出 = x

x ≤ 0：输出 = 0

优点：

计算极快：只需比较大小

缓解梯度消失：正区间梯度恒为1

稀疏激活（部分神经元输出0）

缺点：

神经元死亡：负区间梯度0，参数永不更新

不是零中心化

梯度公式：

text
ReLU'(x) = 1  (x > 0)
ReLU'(x) = 0  (x ≤ 0)
```

##### （4）Leaky ReLU:LeakyReLU(x) = max(αx, x)，α通常取0.01

目的：解决ReLU死亡问题，给负区间一个小梯度

##### （5）Softmax（专用于多分类输出层）:softmax(z_i) = e^{z_i} / Σⱼ₌₁ᴷ e^{z_j}

特点：

输出和为1，形成概率分布

放大差异（指数运算）

只用于输出层

#### 激活函数选择指南


##### 应用场景	         推荐激活函数         原因

隐藏层（默认）	   ReLU	               计算快，效果好

隐藏层（防死亡） 	 Leaky ReLU	         解决神经元死亡

二分类输出层	     Sigmoid	           输出概率

多分类输出层	     Softmax	           输出概率分布

回归输出层	       无/线性	             输出任意实数

RNN/LSTM	       Tanh	               控制信息流动

#### 特征
特征 = 对原始数据的一种 “抽象表示”

原始数据：图片、声音、文字、表格

特征：把原始数据变成模型能理解的数字

## 反向传播
反向传播是一种高效计算梯度的算法，它利用链式法则从输出层向输入层逐层计算损失函数对每个参数的偏导数。

核心思想：误差从输出层反向传播，每一层根据误差调整自己的参数。

### 损失函数

衡量模型预测值与真实值之间的差距：L = f(y_pred, y_true)

#### 回归问题的损失函数

（1）均方误差（MSE）：L = (ŷ - y)²

（2）平均绝对误差（MAE）：L = |ŷ - y|

#### 分类问题的损失函数

（1）二分类交叉熵 ：L = -[y·log(ŷ) + (1-y)·log(1-ŷ)]

（2）多分类交叉熵 ：L = -Σ y_i·log(ŷ_i)

优雅性质：无论分类数多少，输出层的梯度都是预测值与真实值的差。

##### 反向传播的直观理解

1.责任分配思想

反向传播的本质是*责任分配*：

输出层的误差由所有前面的层共同造成

每个参数承担的责任大小 = *它对输出的影响程度 × 输出误差*

梯度就是"责任"的数学度量

***前向传播：信息从输入流向输出 /
反向传播：误差从输出流回输入***

### 梯度消失

到底是什么？梯度消失 = 误差传不回前面的层，前面的层学不到东西，网络训不动。

为什么层数越深，梯度越容易消失？链式法则要求梯度 “一层一层相乘”，层数越深，乘的项越多。每一项都是小于 1 的小数，乘到最前面，梯度变成 0，即梯度消失

### 梯度爆炸

## 监督学习、无监督学习、半监督学习

### 监督学习

定义：训练数据包含输入和对应的标签，模型学习从输入到标签的映射。

```python
HOW TO UNDERSTAND
1. 输入 = 你看到的东西：
图片的像素
房子的面积、房间数、楼层
一个人的身高、体重
一句话的文字
这些叫：X
2. 标签 = 正确答案
这张图是 “猫”
这个房子价格 “200 万”
这个人是 “男性”
这句话是 “正面情绪”
这些叫：y
3. 训练数据 = 一套一套的「题目 + 答案」
（题目 1，答案 1）
（题目 2，答案 2）
（题目 3，答案 3）
…
就是：(X₁, y₁), (X₂, y₂), (X₃, y₃) …
重点来了：
什么叫「学习从输入到标签的映射」？
人话翻译：
模型在背 “规律”，不是背题目。
不是死记硬背每一道题
而是学会：
看到 X → 自动推出 y
这个 X → y 的规律，就叫：
映射（Mapping）
```
监督学习主要分为两种：

【1】分类问题【2】回归问题

### 无监督学习

定义：训练数据只有输入，没有标签，模型自主发现数据中的结构或模式。

无监督学习主要类型：

【1】聚类【2】降维【3】关联规则学习【4】密度估计

### 半监督学习

定义：训练数据中少量有标签，大量无标签，利用无标签数据提升模型性能。

### 三种学习方式的对比
```python
维度	    监督学习	     无监督学习	     半监督学习
数据需求	  全部有标签	   全部无标签	    少量有标签+大量无标签
人工成本	  高（标注）	   低	            中等
目标任务	  预测	       发现结构	      预测+利用结构
输出	    标签/值	     簇/低维表示	    标签
评估	    直接	       间接（聚类质量）	直接（测试集）
复杂度  	  中等	       高（难以评估）	高（算法复杂）
常见算法	  神经网络、SVM	K-Means、PCA	自训练、FixMatch
```

